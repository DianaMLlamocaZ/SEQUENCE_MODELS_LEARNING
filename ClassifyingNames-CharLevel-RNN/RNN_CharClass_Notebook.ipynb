{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports de librerías y configuración de device\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "naDWJNX-R6JA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sSGgdTFXO_6m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "d_-q29OdRWrd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device para entrenar el modelo\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_default_device(device)"
      ],
      "metadata": {
        "id": "O602My-QQV6H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip la data\n",
        "!unzip /content/data_cc.zip -d /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciZwLFp6Q9QS",
        "outputId": "7da79119-be15-4fd1-ea7f-6654d5cbbe99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data_cc.zip\n",
            "   creating: /content/data/\n",
            "  inflating: /content/data/eng-fra.txt  \n",
            "   creating: /content/data/names/\n",
            "  inflating: /content/data/names/Arabic.txt  \n",
            "  inflating: /content/data/names/Chinese.txt  \n",
            "  inflating: /content/data/names/Czech.txt  \n",
            "  inflating: /content/data/names/Dutch.txt  \n",
            "  inflating: /content/data/names/English.txt  \n",
            "  inflating: /content/data/names/French.txt  \n",
            "  inflating: /content/data/names/German.txt  \n",
            "  inflating: /content/data/names/Greek.txt  \n",
            "  inflating: /content/data/names/Irish.txt  \n",
            "  inflating: /content/data/names/Italian.txt  \n",
            "  inflating: /content/data/names/Japanese.txt  \n",
            "  inflating: /content/data/names/Korean.txt  \n",
            "  inflating: /content/data/names/Polish.txt  \n",
            "  inflating: /content/data/names/Portuguese.txt  \n",
            "  inflating: /content/data/names/Russian.txt  \n",
            "  inflating: /content/data/names/Scottish.txt  \n",
            "  inflating: /content/data/names/Spanish.txt  \n",
            "  inflating: /content/data/names/Vietnamese.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento de los caracteres"
      ],
      "metadata": {
        "id": "NqqOoB9kSB2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento de las palabras"
      ],
      "metadata": {
        "id": "kBSXYkX56wpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Caracteres permitidos --> \"_\" representará a cualquier otra letra no conocida\n",
        "chars_allowed=string.ascii_letters+\" .,;'\"+\"_\""
      ],
      "metadata": {
        "id": "XEFU8nlD5I1g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tamaño del vocabulario (servirá para determinar el tamaño de cada vector en OHE)\n",
        "len_chars=len(chars_allowed)"
      ],
      "metadata": {
        "id": "Frl8WtOv6Nmg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para transformar las palabras con signos distintos a caracteres conocidos\n",
        "def oracion_to_ready(orac):\n",
        "  sentence=\"\"\n",
        "  oracion_norm=unicodedata.normalize(\"NFD\",orac)\n",
        "\n",
        "  for char in oracion_norm:\n",
        "    category=unicodedata.category(char)\n",
        "    if category!=\"Mn\":\n",
        "      sentence+=char\n",
        "\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "7ADt-K2Iepbo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convirtiendo a tensor"
      ],
      "metadata": {
        "id": "zCxYwbh86zY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función que devuelve el índice para crear el vector OHE\n",
        "def letter_index(letra):\n",
        "  if letra not in chars_allowed:\n",
        "    return chars_allowed.find(\"_\")\n",
        "\n",
        "  else:\n",
        "    return chars_allowed.find(letra)"
      ],
      "metadata": {
        "id": "dNmZm7z1epZJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Función que crea el vector OHE (el index representará dónde estará el 1 para la notación OHE)\n",
        "def word_to_tensor(palabra):\n",
        "  tensor_final=torch.zeros(size=(len(palabra),1,len_chars))\n",
        "\n",
        "  palabra=oracion_to_ready(palabra) #normalización de la palabra\n",
        "\n",
        "  for i,char in enumerate(palabra):\n",
        "    index=letter_index(char)\n",
        "    tensor_final[i,0,index]=1 #Coloco el 1 para la notación OHE\n",
        "\n",
        "  return tensor_final"
      ],
      "metadata": {
        "id": "hx68rAxg73EP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruebas del 'word_to_tensor' que se usa en cada palabra en el dataset personalizado"
      ],
      "metadata": {
        "id": "B0zRsHOZPfr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res=word_to_tensor(\"café\") #café"
      ],
      "metadata": {
        "id": "oNa0d38WOrTE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(res,dim=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzBLZI2mOuuL",
        "outputId": "ddf03ab4-3525-4576-bdaa-035609a22101"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2],\n",
              "        [0],\n",
              "        [5],\n",
              "        [4]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_allowed[2],chars_allowed[0],chars_allowed[5],chars_allowed[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw1vTqWQOx1F",
        "outputId": "285b1121-e794-441a-e131-d0244aef5c57"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('c', 'a', 'f', 'e')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación del dataset personalizado"
      ],
      "metadata": {
        "id": "HbMAM-WsBQbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "16HozEiICLHN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset personalizado\n",
        "class NamesDataset(Dataset):\n",
        "  #Constructor\n",
        "  def __init__(self,datadir):\n",
        "    self.datadir=datadir\n",
        "\n",
        "    self.data=[]\n",
        "    self.labels=[]\n",
        "\n",
        "    self.labels_total=set()\n",
        "\n",
        "    text_archivos=glob.glob(os.path.join(datadir,\"*.txt\")) #Archivos que terminen en .txt dentro del path indicado\n",
        "\n",
        "    #Bucle para recorrer cada uno de los archivos\n",
        "    for archivo in text_archivos:\n",
        "      palabras=open(archivo).read().strip().split('\\n') #Listas de nombres (palabras)\n",
        "\n",
        "      label=os.path.splitext(os.path.basename(archivo))[0] #Obtengo el label desde el nombre del archivo\n",
        "      self.labels_total.add(label)\n",
        "\n",
        "      for palabra in palabras:\n",
        "        self.data.append(word_to_tensor(palabra))\n",
        "        self.labels.append(label)\n",
        "\n",
        "  #Len\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "  #Get item\n",
        "  def __getitem__(self,idx):\n",
        "    data_idx=self.data[idx]\n",
        "    label_idx=self.labels[idx]\n",
        "\n",
        "    return data_idx,label_idx"
      ],
      "metadata": {
        "id": "D1qpja4hBVB3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=NamesDataset(\"data/names\")"
      ],
      "metadata": {
        "id": "bJCTvCvaBU_R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labels numéricas de las clases"
      ],
      "metadata": {
        "id": "ICBzD7V8mJhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict={label:index for index,label in enumerate(data_total.labels_total)}"
      ],
      "metadata": {
        "id": "m4b49oUumLsy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=torch.tensor(range(0,len(data_total.labels_total)),dtype=torch.long)"
      ],
      "metadata": {
        "id": "0LQ5-5JenR09"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruebitas del dataset class personalizado"
      ],
      "metadata": {
        "id": "Oo4fqlKtNOk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_total),len(data_total.labels_total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR8lwTtcLsJy",
        "outputId": "0b9adc05-c8e3-4985-d6cb-8ed8341f937c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20074, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_total[1] #Vector OHE procesado"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcV1O76uKl1s",
        "outputId": "4679524d-2c2c-4e8b-d990-e68d146982ee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0.]],\n",
              " \n",
              "         [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0.]]]),\n",
              " 'Russian')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.argmax(data_total.data[1],dim=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-W3e5abHGCt",
        "outputId": "fae19cc7-fb0e-4728-d8e4-1e13d9ebd45a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[26],\n",
            "        [ 1],\n",
            "        [ 0],\n",
            "        [ 4],\n",
            "        [21]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Nombre\n",
        "chars_allowed[26],chars_allowed[1],chars_allowed[0],chars_allowed[4],chars_allowed[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hciwPGEhHfo7",
        "outputId": "bdfff317-6cec-4ac6-bcbf-284c4d9477e8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('A', 'b', 'a', 'e', 'v')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# División de datos"
      ],
      "metadata": {
        "id": "vhn6t9AzNb8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set,test_set=torch.utils.data.random_split(data_total,[.85, .15],generator=torch.Generator(device=device).manual_seed(2024))"
      ],
      "metadata": {
        "id": "A3Yr2cu_NdYX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Len train_set:\",len(train_set), \"Len test set:\",len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq9X2k8CRe7S",
        "outputId": "b71fabed-667b-4980-f5d9-5db5122ef6d9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len train_set: 17063 Len test set: 3011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación de la red"
      ],
      "metadata": {
        "id": "D_c_ZT6xSWUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(torch.nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super(CharRNN,self).__init__()\n",
        "\n",
        "    self.rnn=torch.nn.RNN(input_size,hidden_size)\n",
        "    self.h_s=torch.nn.Linear(hidden_size,output_size)\n",
        "    self.lsm=torch.nn.LogSoftmax(dim=1) #convierte los logs a probabilidades\n",
        "\n",
        "  def forward(self,x_data):\n",
        "    rnn_out,hidden=self.rnn(x_data) #rnn_out (resultado de cada time step), hidden (estado oculto final)\n",
        "    #print(\"hidden shape:\",hidden.shape)\n",
        "    output=self.h_s(hidden[0]) #logits sin convertir en probs (hidden[0]), ya que inic. el vector 'hidden' es de shape (1,1,128), y se quiere (1,128) para la capa fully connected)\n",
        "    output=self.lsm(output) #logits convertidos en probs y aplicado su logaritmo de cada prob\n",
        "\n",
        "    return output #rnn_out,hidden,output"
      ],
      "metadata": {
        "id": "6IALg2yqepK6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_modelo=CharRNN(len_chars,128,len(data_total.labels_total))"
      ],
      "metadata": {
        "id": "l1hXhIIIepIx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ60q_O6Vq2b",
        "outputId": "0e6e3794-c9a6-46f3-8abd-fcba05fad8ac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharRNN(\n",
              "  (rnn): RNN(58, 128)\n",
              "  (h_s): Linear(in_features=128, out_features=18, bias=True)\n",
              "  (lsm): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "oKcAUAdJWI-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "O_51sIj1Vq0A"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(rnn,training_data,n_epoch=10,n_batch_size=64,learning_rate=0.2,criterion=torch.nn.NLLLoss()):\n",
        "  optimizer=torch.optim.Adam(rnn.parameters(),0.005) #optimizer\n",
        "\n",
        "  loss_total=[]\n",
        "\n",
        "  #Entrenamiento por dicha cantidad de épocas(igual debe haber un bucle interno para iterar sobre los batches)\n",
        "  for epoca in range(n_epoch):\n",
        "\n",
        "    loss_epoca=0\n",
        "\n",
        "    num_batches=len(training_data)//n_batch_size\n",
        "\n",
        "    idxs_batch_sample=list(range(len(training_data))) #lista de índices de cada sample del batch\n",
        "    batches=np.array_split(idxs_batch_sample,num_batches) #separo en una lista de índices (cada una de tamaño del num_batches)\n",
        "\n",
        "    #print(\"batches\",batches)\n",
        "    #print(f\"num_batches: {num_batches}\")\n",
        "\n",
        "    #Itero sobre cada batch (a su vez estos van a tener los índices de cada palabras, así que también se iterará sobre cada índice de la palabra para pasarle a la RNN)\n",
        "    for idx_batch,batch in enumerate(batches):\n",
        "      loss_batch=0 #loss para calcular el gradiente\n",
        "\n",
        "      #print(f\"idx_batch: {idx_batch}, len batch: {len(batch)}\")\n",
        "      for sample in batch: #Itero sobre cada muestra del batch\n",
        "\n",
        "        tensor_word_data,label_data=training_data[sample]\n",
        "\n",
        "        label=labels[class_dict[label_data]] #label correcta\n",
        "        y_pred=rnn(tensor_word_data) #predicción\n",
        "\n",
        "        loss=criterion(y_pred,label.unsqueeze(0))\n",
        "\n",
        "        loss_batch+=loss\n",
        "        #print(\"loss_batch:\",loss_batch)\n",
        "        #print(\"tensor_word_data shape:\",tensor_word_data.shape)\n",
        "        #print(\"label_data:\",label_data,class_dict[label_data],label,label.unsqueeze(0).shape) #class_dict\n",
        "\n",
        "\n",
        "      optimizer.zero_grad() #Setteo los gradientes a 0 antes de calcular los gradientes de los pesos mediante la loss_batch\n",
        "\n",
        "      loss_batch.backward() #Calculo los gradientes de los pesos con la pérdida del batch\n",
        "\n",
        "      optimizer.step() #Actualizo los pesos de la red\n",
        "\n",
        "      loss_epoca+=loss_batch.item()/len(batch) #Promedio del loss de cada batch se suma al loss de esa época\n",
        "\n",
        "\n",
        "\n",
        "    #Loss promedio en cada época\n",
        "    loss_total.append(loss_epoca/len(batches))\n",
        "\n",
        "    print(f\"Época {epoca}, Loss: {loss_epoca/len(batches)}\")\n",
        "\n",
        "  return loss_total"
      ],
      "metadata": {
        "id": "VQBj4EQGaBMw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_final=train(rnn_modelo,train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah_HQsEh4cvQ",
        "outputId": "a3cfd6ee-ec6a-4bb0-e59e-6a4b6f99786c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 0, Loss: 1.2199156235862563\n",
            "Época 1, Loss: 0.9265171012914961\n",
            "Época 2, Loss: 0.8386138929322388\n",
            "Época 3, Loss: 0.7645445569479238\n",
            "Época 4, Loss: 0.7211768871980158\n",
            "Época 5, Loss: 0.6841448155828057\n",
            "Época 6, Loss: 0.644633860119062\n",
            "Época 7, Loss: 0.6286840989935805\n",
            "Época 8, Loss: 0.6137491022880329\n",
            "Época 9, Loss: 0.5802114986051294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruebas de la red\n",
        "Pruebas de clasificación de idioma dado un nombre"
      ],
      "metadata": {
        "id": "Hg9sG0V3J6bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plb=word_to_tensor(palabra=\"Ramirez\")"
      ],
      "metadata": {
        "id": "-bkQwSk4HbuU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QqcEAVHHhok",
        "outputId": "25bf530e-a6ed-4f17-bbf3-ad697e5a360e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7, 1, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clasificación\n",
        "with torch.no_grad():\n",
        "  pred=rnn_modelo(plb)\n",
        "  clase_pred=torch.argmax(pred,dim=1)\n",
        "\n",
        "print(f\"Clase predicha (tensor): {clase_pred}. Diccionario: {class_dict}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uelpqvqYHknK",
        "outputId": "510db27b-0231-4540-a497-b2fae7e1b6a3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase predicha (tensor): tensor([7]). Diccionario: {'English': 0, 'Vietnamese': 1, 'Polish': 2, 'Czech': 3, 'Scottish': 4, 'Japanese': 5, 'Irish': 6, 'Spanish': 7, 'Greek': 8, 'Korean': 9, 'German': 10, 'Portuguese': 11, 'Arabic': 12, 'Chinese': 13, 'Italian': 14, 'Dutch': 15, 'French': 16, 'Russian': 17}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Clase predicha por el modelo: {list(class_dict.keys())[clase_pred.item()]}\")"
      ],
      "metadata": {
        "id": "rG6qbfZSHkik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83129f1-8fd2-407e-b533-770dbd390ffe"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase predicha por el modelo: Spanish\n"
          ]
        }
      ]
    }
  ]
}