{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u3yYUWDP031",
        "outputId": "19165521-b3e2-4a23-981e-e1e77c5c7972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data_cc.zip\n",
            "   creating: /content/data/\n",
            "  inflating: /content/data/eng-fra.txt  \n",
            "   creating: /content/data/names/\n",
            "  inflating: /content/data/names/Arabic.txt  \n",
            "  inflating: /content/data/names/Chinese.txt  \n",
            "  inflating: /content/data/names/Czech.txt  \n",
            "  inflating: /content/data/names/Dutch.txt  \n",
            "  inflating: /content/data/names/English.txt  \n",
            "  inflating: /content/data/names/French.txt  \n",
            "  inflating: /content/data/names/German.txt  \n",
            "  inflating: /content/data/names/Greek.txt  \n",
            "  inflating: /content/data/names/Irish.txt  \n",
            "  inflating: /content/data/names/Italian.txt  \n",
            "  inflating: /content/data/names/Japanese.txt  \n",
            "  inflating: /content/data/names/Korean.txt  \n",
            "  inflating: /content/data/names/Polish.txt  \n",
            "  inflating: /content/data/names/Portuguese.txt  \n",
            "  inflating: /content/data/names/Russian.txt  \n",
            "  inflating: /content/data/names/Scottish.txt  \n",
            "  inflating: /content/data/names/Spanish.txt  \n",
            "  inflating: /content/data/names/Vietnamese.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/data_cc.zip -d /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SxKIL916LyB"
      },
      "source": [
        "# Importando las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QVZUlKNud5b9"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dWJekFH66bNs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ni7VDTff6dx_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "esNJEPlF_u6X"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krJ_sE7D9nQ-"
      },
      "source": [
        "# Cargando los data files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWnDlaM0_Q99"
      },
      "source": [
        "#### Esta clase se encargará de manejar la data (word 2 index), así como el recuento de las mismas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "apYPEMhh_i3P"
      },
      "outputs": [],
      "source": [
        "#Índices de los tokens (los codificaré en OHE)\n",
        "sos_token=0\n",
        "eos_token=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SUCotHBu_ApW"
      },
      "outputs": [],
      "source": [
        "class Idioma():\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.n_words=2 #2 inicialmente por los tokens sos y eos. Así, el index de la palabra inicial será 2 --> 0 y 1 están ocupadas por SOS Y EOS\n",
        "\n",
        "    self.word2index={} #Mapear a sus índices para crear el OHE por palabra\n",
        "    self.index2word={0:\"SOS\",1:\"EOS\"}\n",
        "    self.word2count={}\n",
        "\n",
        "  def addSentence(self,sentence): #Función para iterar por las palabras dada una oración\n",
        "    for word in sentence.split(\" \"):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self,word): #Función para crear los dicts (y mapear así con sus índices correspondientes)\n",
        "    if word not in self.word2index: #Si NO está esa palabra en el dict, que la cree\n",
        "\n",
        "      self.word2index[word]=self.n_words #Word to index\n",
        "      self.index2word[self.n_words]=word #Index to word\n",
        "      self.word2count[word]=1 #Dict que almacena el recuento. Como esa palabra es nueva ---> count=1\n",
        "\n",
        "      self.n_words+=1 #Actualizo n_words (pues servirá como índice para otras palabras) :D\n",
        "\n",
        "    else: #De lo contrario, si ya existe, solo modifico el dict que lleva el recuento de esa palabra\n",
        "      self.word2count[word]+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6rvQbQRDXc8"
      },
      "source": [
        "#### Preprocesamiento de la data: Unicode a ASCII, remover puntuaciones y demás"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b9h2DbHQDvU3"
      },
      "outputs": [],
      "source": [
        "#Unicode to ascii --> Aquí se realiza la descomposición de caracteres unicode a ascii\n",
        "def Unicode2Ascii(word):\n",
        "  return \"\".join(\n",
        "      c for c in unicodedata.normalize(\"NFD\",word)\n",
        "      if unicodedata.category(c)!=\"Mn\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iWdh4iW_FKzn"
      },
      "outputs": [],
      "source": [
        "#Normalizar las oraciones: A minúscula, eliminar espacios en blancos al inicio o final, etc\n",
        "def normalizeString(s):\n",
        "  s=Unicode2Ascii(s.lower().strip()) #Minúscula, eliminar esp. blancos al inicio o final\n",
        "\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s) #Reemplaza los . ! ? por un 'espacio antes. Es decir hola! -> hola ! (pues \"(white space)\\1\"), donde \\1 hace referencia a ese signo\n",
        "  s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s) #Los que NO sean letras y signos como ! ? -> Se remplazan por un espacio\n",
        "  return s.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmXSlYMdY8Nk"
      },
      "source": [
        "####  Almacenar la data en 'pairs'.\n",
        "Nota:\n",
        "- 'reverse=False' indica que la traducción va\n",
        "   English --> Other Language\n",
        "- 'reverse=True'\n",
        "   Other language --> English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7dVpftxnP63k"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "  #Lista de pairs\n",
        "  pairs=[]\n",
        "\n",
        "  #Leer el archivo y dividir (por líneas) las oraciones\n",
        "  lineas=open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "    read().strip().split(\"\\n\") #Divido en línea las oraciones\n",
        "\n",
        "  #Separando capa 'pair' --> split(\"\\n\")\n",
        "  for index,linea in enumerate(lineas): #itero sobre cada pair\n",
        "\n",
        "    #Obtengo las oraciones de cada pair\n",
        "    ora1,ora2=linea.split(\"\\t\")\n",
        "\n",
        "    #Normalizo las oraciones de cada pair\n",
        "    ora1_norm,ora2_norm=normalizeString(ora1),normalizeString(ora2)\n",
        "\n",
        "    #Agrego los pairs en una lista cada uno\n",
        "    pairs.append([ora1_norm,ora2_norm])\n",
        "\n",
        "  #pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lineas]\n",
        "\n",
        "  #Si reverse=True --> inglés index 1:\n",
        "  if reverse:\n",
        "    pairs=[list(reversed(p)) for p in pairs]\n",
        "\n",
        "    #Instancio objetos de la clase Idioma para cada uno de los languages\n",
        "    input_idioma=Idioma(lang2)\n",
        "    target_idioma=Idioma(lang1)\n",
        "\n",
        "  #Si reverse=False --> inglés index 0:\n",
        "  else:\n",
        "    input_idioma=Idioma(lang1)\n",
        "    target_idioma=Idioma(lang2)\n",
        "\n",
        "\n",
        "  return input_idioma,target_idioma,pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEdPV2hhvJJ4"
      },
      "source": [
        "#### Filtrar algunos tipos de oraciones:\n",
        "- Oraciones con menos de 10 palabras\n",
        "- Oraciones que empiecen con los prefijos indicados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "G1J9DgKFvV4d"
      },
      "outputs": [],
      "source": [
        "#Max length de las oraciones\n",
        "max_length=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-l50kjpDvYtt"
      },
      "outputs": [],
      "source": [
        "#Prefijos con los que deben iniciar las oraciones a entrenar\n",
        "prefijos=(\n",
        "    \"i am\", \"i m\",\n",
        "    \"he is\", \"he s\",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re\",\n",
        "    \"we are\", \"we re\",\n",
        "    \"they are\", \"they re\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X7j3SpCkvzam"
      },
      "outputs": [],
      "source": [
        "#Función para filtrar el tipo de oraciones especificadas\n",
        "def filtrarPair(p):\n",
        "  return len(p[0].split(' ')) < max_length and \\\n",
        "      len(p[1].split(' ')) < max_length and \\\n",
        "      p[1].startswith(prefijos)  #Cuidado con pair[0].startswith, porque 'reverse' puede ser True y NO ser el inglés el index=0, sino 1 OJOO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5OpLBdv3v8UO"
      },
      "outputs": [],
      "source": [
        "#Función que pasa cada par por la función filtrarPair\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filtrarPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8RiN4ucyDgt"
      },
      "source": [
        "#### Preparar la data completa\n",
        "- Para ello, uso las funciones creadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukr1TxX0zcb2"
      },
      "source": [
        "- Formato: [['i m', 'j ai ans'],\n",
        " ['i m ok', 'je vais bien'],\n",
        " ['i m ok', 'ca va']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zrHPjB3kyKTY"
      },
      "outputs": [],
      "source": [
        "def PrepararData(idioma1,idioma2,reverse=False):\n",
        "  #Creo los pares y objetos de las clases\n",
        "  idioma_input,idioma_target,pares=readLangs(idioma1,idioma2,reverse)\n",
        "  print(\"Read %s sentence pairs\" % len(pares))\n",
        "  #Pares filtrados\n",
        "  pares=filterPairs(pares)\n",
        "  #print(f\"pairs: {pairs}\")\n",
        "\n",
        "  #Recorro cada uno de los pares filtrados y uso los objetos creados\n",
        "  for par in pares:\n",
        "    idioma_input.addSentence(par[0])\n",
        "    idioma_target.addSentence(par[1])\n",
        "\n",
        "\n",
        "  #Prints necesarios\n",
        "  print(f\"Idioma: {idioma_input.name}, Número de palabras palabras: {idioma_input.n_words}\")\n",
        "  print(f\"Idioma: {idioma_target.name}, Número de palabras: {idioma_target.n_words}\")\n",
        "\n",
        "  return idioma_input,idioma_target,pares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbEQtKhC7Gjc"
      },
      "source": [
        "# Arquitectura del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb3nKJ0b2UJn"
      },
      "source": [
        "### Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uSgVe0566tom"
      },
      "outputs": [],
      "source": [
        "#Encoder Architecture: GRU\n",
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,dropout_p=0.1):\n",
        "    super(EncoderRNN,self).__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "\n",
        "    self.embedding=nn.Embedding(num_embeddings=input_size,embedding_dim=hidden_size) #Capa de embedding para transformar a vectores los inputs\n",
        "    self.gru=nn.GRU(input_size=hidden_size,hidden_size=hidden_size,batch_first=True) #GRU layer que recibe un vector de size == hidden_size (por la embedding layer)\n",
        "    self.dropout=nn.Dropout(dropout_p) #Dropout para prevenir el overfitting\n",
        "\n",
        "  def forward(self,input):\n",
        "    embedded=self.dropout(self.embedding(input)) #Le paso el conjunto de 'tokens' por batch para que la embedding layer le asigne un vector representativo\n",
        "    output,hidden=self.gru(embedded) #Aquí calculo los hidden states de cada time step (output) y el context vector (hidden state en el último time step)\n",
        "\n",
        "    return output,hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFe2326V5NF_"
      },
      "source": [
        "### Decoder Architecture\n",
        "- El decoder recibirá como input el \"context vector\" (hidden state, del último time step, que contiene una representación compacta de 'toda' la información), además del token inicial BOS que indica el inicio de la oración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ggZHo46U6tk2"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,output_size):\n",
        "    super(DecoderRNN,self).__init__()\n",
        "\n",
        "    self.embedding=nn.Embedding(num_embeddings=output_size,embedding_dim=hidden_size) #Embedding layer\n",
        "    self.gru=nn.GRU(hidden_size,hidden_size,batch_first=True) #GRU Layer\n",
        "    self.out=nn.Linear(hidden_size,output_size) #Esta es una capa lineal a la que se le aplicará la softmaxt act. function\n",
        "\n",
        "  def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
        "    batch_size=encoder_outputs.size(0) #Obtengo el batch_size\n",
        "    decoder_input=torch.empty(batch_size,1,dtype=torch.long,device=device).fill_(sos_token) #El input inicial del decoder será el token sos (start of sentence)\n",
        "    decoder_hidden=encoder_hidden #El hidden state inicial del DECODER será el hidden state del último time step del encoder\n",
        "    decoder_outputs=[]\n",
        "\n",
        "    #Forward pass (por cada palabra en una oración): Recibe el de SOS token y el hidden state del último time step del encoder\n",
        "    for i in range(encoder_outputs.size(1)): #max_length\n",
        "      decoder_output,decoder_hidden=self.forward_step(decoder_input,decoder_hidden)\n",
        "      decoder_outputs.append(decoder_output)\n",
        "\n",
        "      #Teacher forcing\n",
        "      if target_tensor is not None:\n",
        "        decoder_input=target_tensor[:,i].unsqueeze(1) #Al usar 'teacher forcing', el target se pasa como input 'siguiente' al decoder\n",
        "\n",
        "      #Sin teacher forcing: Usa sus 'propias' predicciones para predecir el siguiente input\n",
        "      else:\n",
        "        _,topi=decoder_output.topk(1) #Aquí obtengo solo el índice del elemento con mayor valor (y el logit)\n",
        "        decoder_input=topi.squeeze(-1).detach() #'detach' porque se sobreentiende que es el input siguiente\n",
        "\n",
        "\n",
        "    decoder_outputs=torch.cat(decoder_outputs,dim=1)\n",
        "    decoder_outputs=F.log_softmax(decoder_outputs,dim=-1) #Calculo el \"log de las probabilidades\" a partir de los 'logits' predichos por el modelo\n",
        "\n",
        "    return decoder_outputs,decoder_hidden,None\n",
        "\n",
        "  def forward_step(self,input,hidden):\n",
        "    output=self.embedding(input) #el input pasa por el embedding layer\n",
        "    output=F.relu(output) #relu activation en el output (embedding)\n",
        "    output,hidden=self.gru(output,hidden)\n",
        "    output=self.out(output)\n",
        "\n",
        "    return output,hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder y Bahdanau Attention"
      ],
      "metadata": {
        "id": "bGS32DnSvJzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "  def __init__(self,hidden_size):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.Wa=nn.Linear(hidden_size,hidden_size)\n",
        "    self.Ua=nn.Linear(hidden_size,hidden_size)\n",
        "    self.Va=nn.Linear(hidden_size,1)\n",
        "\n",
        "  def forward(self,query,keys):\n",
        "    scores=self.Va(torch.tanh(self.Wa(query)+self.Ua(keys)))\n",
        "    scores=scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "    weights=F.softmax(scores,dim=1)\n",
        "    context=torch.bmm(weights,keys)\n",
        "\n",
        "    return context,weights"
      ],
      "metadata": {
        "id": "VhRGmDR1gdyC"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,output_size,dropout_p=0.1):\n",
        "    super(AttnDecoderRNN,self).__init__()\n",
        "    self.embedding=nn.Embedding(output_size,hidden_size)\n",
        "    self.attention=BahdanauAttention(hidden_size)\n",
        "    self.gru=nn.GRU(2*hidden_size,hidden_size,batch_first=True)\n",
        "    self.out=nn.Linear(hidden_size,output_size)\n",
        "    self.dropout=nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
        "    batch_size=encoder_outputs.size(0)\n",
        "    decoder_input=torch.empty(batch_size,1,dtype=torch.long,device=device).fill_(sos_token)\n",
        "    decoder_hidden=encoder_hidden\n",
        "    decoder_outputs=[]\n",
        "    attentions=[]\n",
        "\n",
        "    for i in range(max_length):\n",
        "      decoder_output,decoder_hidden,attn_weights=self.forward_step(decoder_input,decoder_hidden,encoder_outputs)\n",
        "\n",
        "      decoder_outputs.append(decoder_output)\n",
        "      attentions.append(attn_weights)\n",
        "\n",
        "      if target_tensor is not None: #Teacher Forcing\n",
        "        decoder_input=target_tensor[:,i].unsqueeze(1)\n",
        "\n",
        "      else: #No Teacher Forcing --> Predicciones propias del modelo\n",
        "        _,topi=decoder_output.topk(1)\n",
        "        decoder_input=topi.squeeze(-1).detach()\n",
        "\n",
        "    decoder_outputs=torch.cat(decoder_outputs,dim=1)\n",
        "\n",
        "    decoder_outputs=F.log_softmax(decoder_outputs,dim=-1)\n",
        "\n",
        "    attentions=torch.cat(attentions,dim=1)\n",
        "\n",
        "    return decoder_outputs,decoder_hidden,attentions\n",
        "\n",
        "  def forward_step(self,input,hidden,encoder_outputs):\n",
        "    embedded=self.dropout(self.embedding(input)) #convertir a embeddings\n",
        "\n",
        "    query=hidden.permute(1,0,2)\n",
        "    context,attn_weights=self.attention(query,encoder_outputs) #calcular los attention weights y el context vector\n",
        "    input_gru=torch.cat((embedded,context),dim=2) #concatenar el embedded vector y el context vector calculado por los attn weights\n",
        "\n",
        "    output,hidden=self.gru(input_gru,hidden)\n",
        "    output=self.out(output)\n",
        "\n",
        "    return output,hidden,attn_weights"
      ],
      "metadata": {
        "id": "pD-ykg23jgnH"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-j1b1iUOy_R"
      },
      "source": [
        "# Creando la data para entrenar al modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRNdpoMgRZXA"
      },
      "source": [
        "- Función que convierte las oraciones en una lista de índices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wUv2lCLRQhEm"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang,sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcVN7T6LRqHX"
      },
      "source": [
        "- Función que añade el token SOS al final y convierte a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9GiFoJSMRvpg"
      },
      "outputs": [],
      "source": [
        "def tensorFromSentence(lang,sentence):\n",
        "    indexes=indexesFromSentence(lang,sentence) #Convierto la oración a índices\n",
        "    indexes.append(eos_token) #Añado el token EOS\n",
        "    return torch.tensor(indexes,dtype=torch.long,device=device).view(1,-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBYtpaJtSYF_"
      },
      "source": [
        "- Función que mapea un 'pair' (input-target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "N3FKagypOt-o"
      },
      "outputs": [],
      "source": [
        "def tensorsFromPair(pair):\n",
        "    input_tensor=tensorFromSentence(input_lang,pair[0]) #El primer elemento del pair es el input lang\n",
        "    target_tensor=tensorFromSentence(output_lang,pair[1]) #El segundo elemento del pair es el output lang\n",
        "    return (input_tensor,target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypoKGXdeVFnO"
      },
      "source": [
        "- Función para crear el dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UR6-lA9UVIZ2"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(batch_size):\n",
        "  input_lang,output_lang,pairs=PrepararData(\"eng\",\"fra\",True)\n",
        "\n",
        "  n=len(pairs) #Cantidad de oraciones\n",
        "\n",
        "  input_ids=np.zeros((n,max_length),dtype=np.int32) #Inicialmente, los vectores input/output id serán vectores de 0\n",
        "  target_ids=np.zeros((n,max_length),dtype=np.int32)\n",
        "\n",
        "  for idx,(inp,tgt) in enumerate(pairs):\n",
        "    inp_ids=indexesFromSentence(input_lang,inp) #Listas de tokens índices del input lang\n",
        "    tgt_ids=indexesFromSentence(output_lang,tgt) #Listas de tokens índices del output lang\n",
        "\n",
        "    inp_ids.append(eos_token) #Añado el token eos\n",
        "    tgt_ids.append(eos_token)\n",
        "\n",
        "    input_ids[idx,:len(inp_ids)]=inp_ids #Al tensor de zeros (input_ids), le coloco los input ids en los primeros \"len(input_ids)\" índices\n",
        "    target_ids[idx,:len(tgt_ids)]=tgt_ids #Igual al tensor de zeros para output_id\n",
        "\n",
        "  #Creo el dataset\n",
        "  train_data=TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                              torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "  train_sampler=RandomSampler(train_data)\n",
        "  train_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  return input_lang,output_lang,train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLJGHqsYU5rc"
      },
      "source": [
        "# Entrenando el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "U0WOybGx6xLb"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader_train,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion):\n",
        "  total_loss = 0\n",
        "\n",
        "  #Itero sobre el dataloader\n",
        "  for data in dataloader_train:\n",
        "    input,target=data\n",
        "\n",
        "    #El input le paso al encoder --> Genera los outputs en cada time step y el hidden state del último time step\n",
        "    encoder_outputs,encoder_hidden=encoder(input)\n",
        "\n",
        "    #El decoder recibe como inputs el encoder_outputs y el último hidden state del encoder y devuelve las log probs (decoder_outputs)\n",
        "    decoder_outputs,decoder_hidden,_=decoder(encoder_outputs,encoder_hidden,target)\n",
        "\n",
        "    #Optimizers a 0 para evitar acumulación de gradientes de pasos anteriores\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    #Calculo el loss\n",
        "    loss=criterion(\n",
        "          decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "          target.view(-1)\n",
        "      )\n",
        "\n",
        "\n",
        "    #Calculo los gradientes de los pesos a partir del loss\n",
        "    loss.backward()\n",
        "\n",
        "    #Actualizo los pesos del modelo en base a esos gradientes\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sjW4uwmaRhno"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  #Se resetea cada print\n",
        "    plot_loss_total = 0  #Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss(ignore_index=0) #Ignorar el pad index\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(f\"Época {epoch}, Loss average: {print_loss_avg}\")\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "MUS9z54fa2sB"
      },
      "outputs": [],
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm-nciQJalp4",
        "outputId": "f4274b4a-e8a9-4838-ce69-8f61dea90d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 135842 sentence pairs\n",
            "Idioma: fra, Número de palabras palabras: 5228\n",
            "Idioma: eng, Número de palabras: 3434\n",
            "Época 5, Loss average: 2.4382833521064398\n",
            "Época 10, Loss average: 1.0220397266797332\n",
            "Época 15, Loss average: 0.5245561697716749\n",
            "Época 20, Loss average: 0.31354739818324523\n",
            "Época 25, Loss average: 0.21543348061091847\n",
            "Época 30, Loss average: 0.16472189253746133\n",
            "Época 35, Loss average: 0.13508095767154882\n",
            "Época 40, Loss average: 0.11775858768125622\n",
            "Época 45, Loss average: 0.10814708167728655\n",
            "Época 50, Loss average: 0.09984495628431034\n",
            "Época 55, Loss average: 0.09371503378280784\n",
            "Época 60, Loss average: 0.08920517302772246\n",
            "Época 65, Loss average: 0.08514927599865656\n",
            "Época 70, Loss average: 0.08221261029411338\n",
            "Época 75, Loss average: 0.08147792295453418\n",
            "Época 80, Loss average: 0.07901172770866685\n"
          ]
        }
      ],
      "source": [
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0NBTyAYZqww"
      },
      "source": [
        "# Evaluación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "YWFA4syTallT"
      },
      "outputs": [],
      "source": [
        "def evaluar(encoder,decoder,oracion,traduccion_real,input_lang,output_lang,length_max):\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    oracion_token=tensorFromSentence(input_lang,oracion) #shape [1,10]: 1 muestra, 10 seq length\n",
        "\n",
        "\n",
        "    #Le paso el vector de tokens al encoder para obtener el hidden state del encoder y pasarle al decoder\n",
        "    output_encoder,hidden_encoder=encoder(oracion_token) #[1,seq_length,dim_hidden] --> [1,10,128]\n",
        "\n",
        "    #Decoder\n",
        "    decoder_outputs,decoder_hidden,decoder_attn=decoder(output_encoder,hidden_encoder)\n",
        "\n",
        "    #Index de la palabra 'predicha' por el modelo\n",
        "    _, topi=decoder_outputs.topk(1)\n",
        "    decoded_ids=topi.squeeze()\n",
        "\n",
        "    #Predicciones del modelo hasta que se genere el EOS token\n",
        "    decoded_words=[]\n",
        "    for idx in decoded_ids:\n",
        "          if idx.item()==eos_token:\n",
        "              decoded_words.append('<EOS>')\n",
        "              break\n",
        "          decoded_words.append(output_lang.index2word[idx.item()])\n",
        "\n",
        "  return decoded_words, decoder_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oraciones de prueba"
      ],
      "metadata": {
        "id": "szj2xWpHM1QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Oración de prueba en francés\n",
        "oraciones=[\"il est en train de peindre un tableau\",\n",
        "           \"il est de loin le meilleur des etudiants\",\n",
        "           \"tu es ambitieuse\",\n",
        "           \"nous sommes tristes\",\n",
        "           \"vous etes chanceuse d avoir un travail\",\n",
        "           \"je suis trop vieille pour ce genre de choses\",\n",
        "           \"elle fait juste semblant\",\n",
        "           \"nous allons toutes a la maison\"]"
      ],
      "metadata": {
        "id": "gRCjynYiKptU"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Traducciones reales de las oraciones\n",
        "oraciones_target=[\"he is painting a picture\",\n",
        "           \"he is by far the best studeint\",\n",
        "           \"you re ambitious\",\n",
        "           \"we re sad\",\n",
        "           \"you re lucky that you have a job\",\n",
        "           \"i m too old for this sort of thing\",\n",
        "          \"she s just putting up a front\",\n",
        "          \"we re all going home\"]"
      ],
      "metadata": {
        "id": "X-T2DLE-RnQs"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "ewO7ufdelR3-"
      },
      "outputs": [],
      "source": [
        "preds=[]\n",
        "for i in range(len(oraciones)):\n",
        "  oracion=oraciones[i]\n",
        "  traduccion_real=oraciones_target[i]\n",
        "\n",
        "  dec_words,_=evaluar(encoder,decoder,oracion,traduccion_real,input_lang,output_lang,10)\n",
        "  preds.append(dec_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imprimo las oraciones predichas\n",
        "for index,oracion_pred in enumerate(preds):\n",
        "  oracion_text=\"\"\n",
        "  for word in oracion_pred:\n",
        "    oracion_text+=word+\" \"\n",
        "\n",
        "  print(f\"Oración a traducir: {oraciones[index]}\")\n",
        "  print(f\"Oración target: {oraciones_target[index]}\")\n",
        "  print(f\"Oración traducida: {oracion_text}\")\n",
        "  print(\"====\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pdhdv2IHq7F",
        "outputId": "231a888b-e66e-43d8-9a22-ed6dc819d1e8"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oración a traducir: il est en train de peindre un tableau\n",
            "Oración target: he is painting a picture\n",
            "Oración traducida: he is painting a picture <EOS> \n",
            "====\n",
            "Oración a traducir: il est de loin le meilleur des etudiants\n",
            "Oración target: he is by far the best studeint\n",
            "Oración traducida: he is by far the best student <EOS> \n",
            "====\n",
            "Oración a traducir: tu es ambitieuse\n",
            "Oración target: you re ambitious\n",
            "Oración traducida: you re probably tired of cute when you re just \n",
            "====\n",
            "Oración a traducir: nous sommes tristes\n",
            "Oración target: we re sad\n",
            "Oración traducida: we re not kind of busy here <EOS> \n",
            "====\n",
            "Oración a traducir: vous etes chanceuse d avoir un travail\n",
            "Oración target: you re lucky that you have a job\n",
            "Oración traducida: you re lucky that you have a job <EOS> \n",
            "====\n",
            "Oración a traducir: je suis trop vieille pour ce genre de choses\n",
            "Oración target: i m too old for this sort of thing\n",
            "Oración traducida: i m too old for this sort of thing <EOS> \n",
            "====\n",
            "Oración a traducir: elle fait juste semblant\n",
            "Oración target: she s just putting up a front\n",
            "Oración traducida: she s putting what she s putting up a face \n",
            "====\n",
            "Oración a traducir: nous allons toutes a la maison\n",
            "Oración target: we re all going home\n",
            "Oración traducida: we re all going to all home all home <EOS> \n",
            "====\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn7xH_dZ6vvj"
      },
      "source": [
        "-----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}