{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u3yYUWDP031",
        "outputId": "9883c6fe-38e3-47a0-bf72-2d2bf626f483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data_cc.zip\n",
            "replace /content/data/eng-fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/data_cc.zip -d /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SxKIL916LyB"
      },
      "source": [
        "# Importando las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QVZUlKNud5b9"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dWJekFH66bNs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ni7VDTff6dx_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "esNJEPlF_u6X"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krJ_sE7D9nQ-"
      },
      "source": [
        "# Cargando los data files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWnDlaM0_Q99"
      },
      "source": [
        "#### Esta clase se encargará de manejar la data (word 2 index), así como el recuento de las mismas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "apYPEMhh_i3P"
      },
      "outputs": [],
      "source": [
        "#Índices de los tokens (los codificaré en OHE)\n",
        "sos_token=0\n",
        "eos_token=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SUCotHBu_ApW"
      },
      "outputs": [],
      "source": [
        "class Idioma():\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.n_words=2 #2 inicialmente por los tokens sos y eos. Así, el index de la palabra inicial será 2 --> 0 y 1 están ocupadas por SOS Y EOS\n",
        "\n",
        "    self.word2index={} #Mapear a sus índices para crear el OHE por palabra\n",
        "    self.index2word={0:\"SOS\",1:\"EOS\"}\n",
        "    self.word2count={}\n",
        "\n",
        "  def addSentence(self,sentence): #Función para iterar por las palabras dada una oración\n",
        "    for word in sentence.split(\" \"):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self,word): #Función para crear los dicts (y mapear así con sus índices correspondientes)\n",
        "    if word not in self.word2index: #Si NO está esa palabra en el dict, que la cree\n",
        "\n",
        "      self.word2index[word]=self.n_words #Word to index\n",
        "      self.index2word[self.n_words]=word #Index to word\n",
        "      self.word2count[word]=1 #Dict que almacena el recuento. Como esa palabra es nueva ---> count=1\n",
        "\n",
        "      self.n_words+=1 #Actualizo n_words (pues servirá como índice para otras palabras) :D\n",
        "\n",
        "    else: #De lo contrario, si ya existe, solo modifico el dict que lleva el recuento de esa palabra\n",
        "      self.word2count[word]+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6rvQbQRDXc8"
      },
      "source": [
        "#### Preprocesamiento de la data: Unicode a ASCII, remover puntuaciones y demás"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b9h2DbHQDvU3"
      },
      "outputs": [],
      "source": [
        "#Unicode to ascii --> Aquí se realiza la descomposición de caracteres unicode a ascii\n",
        "def Unicode2Ascii(word):\n",
        "  return \"\".join(\n",
        "      c for c in unicodedata.normalize(\"NFD\",word)\n",
        "      if unicodedata.category(c)!=\"Mn\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iWdh4iW_FKzn"
      },
      "outputs": [],
      "source": [
        "#Normalizar las oraciones: A minúscula, eliminar espacios en blancos al inicio o final, etc\n",
        "def normalizeString(s):\n",
        "  s=Unicode2Ascii(s.lower().strip()) #Minúscula, eliminar esp. blancos al inicio o final\n",
        "\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s) #Reemplaza los . ! ? por un 'espacio antes. Es decir hola! -> hola ! (pues \"(white space)\\1\"), donde \\1 hace referencia a ese signo\n",
        "  s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s) #Los que NO sean letras y signos como ! ? -> Se remplazan por un espacio\n",
        "  return s.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmXSlYMdY8Nk"
      },
      "source": [
        "####  Almacenar la data en 'pairs'.\n",
        "Nota:\n",
        "- 'reverse=False' indica que la traducción va\n",
        "   English --> Other Language\n",
        "- 'reverse=True'\n",
        "   Other language --> English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7dVpftxnP63k"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "  #Lista de pairs\n",
        "  pairs=[]\n",
        "\n",
        "  #Leer el archivo y dividir (por líneas) las oraciones\n",
        "  lineas=open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "    read().strip().split(\"\\n\") #Divido en línea las oraciones\n",
        "\n",
        "  #Separando capa 'pair' --> split(\"\\n\")\n",
        "  for index,linea in enumerate(lineas): #itero sobre cada pair\n",
        "\n",
        "    #Obtengo las oraciones de cada pair\n",
        "    ora1,ora2=linea.split(\"\\t\")\n",
        "\n",
        "    #Normalizo las oraciones de cada pair\n",
        "    ora1_norm,ora2_norm=normalizeString(ora1),normalizeString(ora2)\n",
        "\n",
        "    #Agrego los pairs en una lista cada uno\n",
        "    pairs.append([ora1_norm,ora2_norm])\n",
        "\n",
        "  #pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lineas]\n",
        "\n",
        "  #Si reverse=True --> inglés index 1:\n",
        "  if reverse:\n",
        "    pairs=[list(reversed(p)) for p in pairs]\n",
        "\n",
        "    #Instancio objetos de la clase Idioma para cada uno de los languages\n",
        "    input_idioma=Idioma(lang2)\n",
        "    target_idioma=Idioma(lang1)\n",
        "\n",
        "  #Si reverse=False --> inglés index 0:\n",
        "  else:\n",
        "    input_idioma=Idioma(lang1)\n",
        "    target_idioma=Idioma(lang2)\n",
        "\n",
        "\n",
        "  return input_idioma,target_idioma,pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEdPV2hhvJJ4"
      },
      "source": [
        "#### Filtrar algunos tipos de oraciones:\n",
        "- Oraciones con menos de 10 palabras\n",
        "- Oraciones que empiecen con los prefijos indicados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G1J9DgKFvV4d"
      },
      "outputs": [],
      "source": [
        "#Max length de las oraciones\n",
        "max_length=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-l50kjpDvYtt"
      },
      "outputs": [],
      "source": [
        "#Prefijos con los que deben iniciar las oraciones a entrenar\n",
        "prefijos=(\n",
        "    \"i am\", \"i m\",\n",
        "    \"he is\", \"he s\",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re\",\n",
        "    \"we are\", \"we re\",\n",
        "    \"they are\", \"they re\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "X7j3SpCkvzam"
      },
      "outputs": [],
      "source": [
        "#Función para filtrar el tipo de oraciones especificadas\n",
        "def filtrarPair(p):\n",
        "  return len(p[0].split(' ')) < max_length and \\\n",
        "      len(p[1].split(' ')) < max_length and \\\n",
        "      p[1].startswith(prefijos)  #Cuidado con pair[0].startswith, porque 'reverse' puede ser True y NO ser el inglés el index=0, sino 1 OJOO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5OpLBdv3v8UO"
      },
      "outputs": [],
      "source": [
        "#Función que pasa cada par por la función filtrarPair\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filtrarPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8RiN4ucyDgt"
      },
      "source": [
        "#### Preparar la data completa\n",
        "- Para ello, uso las funciones creadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukr1TxX0zcb2"
      },
      "source": [
        "- Formato: [['i m', 'j ai ans'],\n",
        " ['i m ok', 'je vais bien'],\n",
        " ['i m ok', 'ca va']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zrHPjB3kyKTY"
      },
      "outputs": [],
      "source": [
        "def PrepararData(idioma1,idioma2,reverse=False):\n",
        "  #Creo los pares y objetos de las clases\n",
        "  idioma_input,idioma_target,pares=readLangs(idioma1,idioma2,reverse)\n",
        "  print(\"Read %s sentence pairs\" % len(pares))\n",
        "  #Pares filtrados\n",
        "  pares=filterPairs(pares)\n",
        "  #print(f\"pairs: {pairs}\")\n",
        "\n",
        "  #Recorro cada uno de los pares filtrados y uso los objetos creados\n",
        "  for par in pares:\n",
        "    idioma_input.addSentence(par[0])\n",
        "    idioma_target.addSentence(par[1])\n",
        "\n",
        "\n",
        "  #Prints necesarios\n",
        "  print(f\"Idioma: {idioma_input.name}, Número de palabras palabras: {idioma_input.n_words}\")\n",
        "  print(f\"Idioma: {idioma_target.name}, Número de palabras: {idioma_target.n_words}\")\n",
        "\n",
        "  return idioma_input,idioma_target,pares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbEQtKhC7Gjc"
      },
      "source": [
        "# Arquitectura del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb3nKJ0b2UJn"
      },
      "source": [
        "### Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uSgVe0566tom"
      },
      "outputs": [],
      "source": [
        "#Encoder Architecture: GRU\n",
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,dropout_p=0.1):\n",
        "    super(EncoderRNN,self).__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "\n",
        "    self.embedding=nn.Embedding(num_embeddings=input_size,embedding_dim=hidden_size) #Capa de embedding para transformar a vectores los inputs\n",
        "    self.gru=nn.GRU(input_size=hidden_size,hidden_size=hidden_size,batch_first=True) #GRU layer que recibe un vector de size == hidden_size (por la embedding layer)\n",
        "    self.dropout=nn.Dropout(dropout_p) #Dropout para prevenir el overfitting\n",
        "\n",
        "  def forward(self,input):\n",
        "    embedded=self.dropout(self.embedding(input)) #Le paso el conjunto de 'tokens' por batch para que la embedding layer le asigne un vector representativo\n",
        "    output,hidden=self.gru(embedded) #Aquí calculo los hidden states de cada time step (output) y el context vector (hidden state en el último time step)\n",
        "\n",
        "    return output,hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFe2326V5NF_"
      },
      "source": [
        "### Decoder Architecture\n",
        "- El decoder recibirá como input el \"context vector\" (hidden state, del último time step, que contiene una representación compacta de 'toda' la información), además del token inicial BOS que indica el inicio de la oración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ggZHo46U6tk2"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_size,output_size):\n",
        "    super(DecoderRNN,self).__init__()\n",
        "\n",
        "    self.embedding=nn.Embedding(num_embeddings=output_size,embedding_dim=hidden_size) #Embedding layer\n",
        "    self.gru=nn.GRU(hidden_size,hidden_size,batch_first=True) #GRU Layer\n",
        "    self.out=nn.Linear(hidden_size,output_size) #Esta es una capa lineal a la que se le aplicará la softmaxt act. function\n",
        "\n",
        "  def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
        "    batch_size=encoder_outputs.size(0) #Obtengo el batch_size\n",
        "    decoder_input=torch.empty(batch_size,1,dtype=torch.long,device=device).fill_(sos_token) #El input inicial del decoder será el token sos (start of sentence)\n",
        "    decoder_hidden=encoder_hidden #El hidden state inicial del DECODER será el hidden state del último time step del encoder\n",
        "    decoder_outputs=[]\n",
        "\n",
        "    #Forward pass (por cada palabra en una oración): Recibe el de SOS token y el hidden state del último time step del encoder\n",
        "    for i in range(encoder_outputs.size(1)): #max_length\n",
        "      decoder_output,decoder_hidden=self.forward_step(decoder_input,decoder_hidden)\n",
        "      decoder_outputs.append(decoder_output)\n",
        "\n",
        "      #Teacher forcing\n",
        "      if target_tensor is not None:\n",
        "        decoder_input=target_tensor[:,i].unsqueeze(1) #Al usar 'teacher forcing', el target se pasa como input 'siguiente' al decoder\n",
        "\n",
        "      #Sin teacher forcing: Usa sus 'propias' predicciones para predecir el siguiente input\n",
        "      else:\n",
        "        _,topi=decoder_output.topk(1) #Aquí obtengo solo el índice del elemento con mayor valor (y el logit)\n",
        "        decoder_input=topi.squeeze(-1).detach() #'detach' porque se sobreentiende que es el input siguiente\n",
        "\n",
        "\n",
        "    decoder_outputs=torch.cat(decoder_outputs,dim=1)\n",
        "    decoder_outputs=F.log_softmax(decoder_outputs,dim=-1) #Calculo el \"log de las probabilidades\" a partir de los 'logits' predichos por el modelo\n",
        "\n",
        "    return decoder_outputs,decoder_hidden,None\n",
        "\n",
        "  def forward_step(self,input,hidden):\n",
        "    output=self.embedding(input) #el input pasa por el embedding layer\n",
        "    output=F.relu(output) #relu activation en el output (embedding)\n",
        "    output,hidden=self.gru(output,hidden)\n",
        "    output=self.out(output)\n",
        "\n",
        "    return output,hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-j1b1iUOy_R"
      },
      "source": [
        "# Creando la data para entrenar al modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRNdpoMgRZXA"
      },
      "source": [
        "- Función que convierte las oraciones en una lista de índices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wUv2lCLRQhEm"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang,sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcVN7T6LRqHX"
      },
      "source": [
        "- Función que añade el token SOS al final y convierte a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9GiFoJSMRvpg"
      },
      "outputs": [],
      "source": [
        "def tensorFromSentence(lang,sentence):\n",
        "    indexes=indexesFromSentence(lang,sentence) #Convierto la oración a índices\n",
        "    indexes.append(eos_token) #Añado el token EOS\n",
        "    return torch.tensor(indexes,dtype=torch.long,device=device).view(1,-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBYtpaJtSYF_"
      },
      "source": [
        "- Función que mapea un 'pair' (input-target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N3FKagypOt-o"
      },
      "outputs": [],
      "source": [
        "def tensorsFromPair(pair):\n",
        "    input_tensor=tensorFromSentence(input_lang,pair[0]) #El primer elemento del pair es el input lang\n",
        "    target_tensor=tensorFromSentence(output_lang,pair[1]) #El segundo elemento del pair es el output lang\n",
        "    return (input_tensor,target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypoKGXdeVFnO"
      },
      "source": [
        "- Función para crear el dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UR6-lA9UVIZ2"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(batch_size):\n",
        "  input_lang,output_lang,pairs=PrepararData(\"eng\",\"fra\",True)\n",
        "\n",
        "  n=len(pairs) #Cantidad de oraciones\n",
        "\n",
        "  input_ids=np.zeros((n,max_length),dtype=np.int32) #Inicialmente, los vectores input/output id serán vectores de 0\n",
        "  target_ids=np.zeros((n,max_length),dtype=np.int32)\n",
        "\n",
        "  for idx,(inp,tgt) in enumerate(pairs):\n",
        "    inp_ids=indexesFromSentence(input_lang,inp) #Listas de tokens índices del input lang\n",
        "    tgt_ids=indexesFromSentence(output_lang,tgt) #Listas de tokens índices del output lang\n",
        "\n",
        "    inp_ids.append(eos_token) #Añado el token eos\n",
        "    tgt_ids.append(eos_token)\n",
        "\n",
        "    input_ids[idx,:len(inp_ids)]=inp_ids #Al tensor de zeros (input_ids), le coloco los input ids en los primeros \"len(input_ids)\" índices\n",
        "    target_ids[idx,:len(tgt_ids)]=tgt_ids #Igual al tensor de zeros para output_id\n",
        "\n",
        "  #Creo el dataset\n",
        "  train_data=TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                              torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "  train_sampler=RandomSampler(train_data)\n",
        "  train_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  return input_lang,output_lang,train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlRspMtAjmwH",
        "outputId": "559faa3f-1ef8-4d67-b7fd-62cdc5f26190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 135842 sentence pairs\n",
            "Idioma: fra, Número de palabras palabras: 5228\n",
            "Idioma: eng, Número de palabras: 3434\n"
          ]
        }
      ],
      "source": [
        "input_lang, output_lang, train_dataloader = get_dataloader(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2KaZptVOt5n",
        "outputId": "4e0e772a-6ab1-4c56-bfea-702f05e177eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1529,   97,    5,   10,   13, 4475,  259,  331, 4476,    1],\n",
            "        [  23,   24, 1067,  266,  259, 2513,    1,    0,    0,    0],\n",
            "        [   5,   10,  116,    5,  330,  125,    3,  272, 4087,    1],\n",
            "        [   5,   10,  954,  306,   69,  125,    1,    0,    0,    0],\n",
            "        [ 383,  582,  583,    1,    0,    0,    0,    0,    0,    0]])\n",
            "tensor([[   2,    3, 2023,   41, 2833,  136,  256,    1,    0,    0],\n",
            "        [  13,   39,  593,   24,  586, 1485,    1,    0,    0,    0],\n",
            "        [   2,    3,   72,    2, 2550,  606, 1360,  139,    1,    0],\n",
            "        [   2,    3, 1027,  180, 1027,  139,  134,    1,    0,    0],\n",
            "        [ 236,  337,    1,    0,    0,    0,    0,    0,    0,    0]])\n",
            "torch.Size([32, 10])\n",
            "torch.Size([32, 10])\n"
          ]
        }
      ],
      "source": [
        "for train in train_dataloader:\n",
        "  print(train[0][:5,:])\n",
        "  print(train[1][:5,:])\n",
        "  targets=train[1]\n",
        "\n",
        "  print(train[0].shape)\n",
        "  print(train[1].shape)\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLJGHqsYU5rc"
      },
      "source": [
        "# Entrenando el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "U0WOybGx6xLb"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader_train,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion):\n",
        "  total_loss = 0\n",
        "\n",
        "  #Itero sobre el dataloader\n",
        "  for data in dataloader_train:\n",
        "    input,target=data\n",
        "\n",
        "    #El input le paso al encoder --> Genera los outputs en cada time step y el hidden state del último time step\n",
        "    encoder_outputs,encoder_hidden=encoder(input)\n",
        "\n",
        "    #El decoder recibe como inputs el encoder_outputs y el último hidden state del encoder y devuelve las log probs (decoder_outputs)\n",
        "    decoder_outputs,decoder_hidden,_=decoder(encoder_outputs,encoder_hidden,target)\n",
        "\n",
        "    #Optimizers a 0 para evitar acumulación de gradientes de pasos anteriores\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    #Calculo el loss\n",
        "    loss=criterion(\n",
        "          decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "          target.view(-1)\n",
        "      )\n",
        "\n",
        "    #Calculo los gradientes de los pesos a partir del loss\n",
        "    loss.backward()\n",
        "\n",
        "    #Actualizo los pesos del modelo en base a esos gradientes\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "sjW4uwmaRhno"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  #Se resetea cada print\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss(ignore_index=0) #Ignorar el pad index\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print(f\"Época {epoch}, Loss average: {print_loss_avg}\")\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MUS9z54fa2sB"
      },
      "outputs": [],
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm-nciQJalp4",
        "outputId": "fac25037-cd74-4aac-9aaa-954672b7c45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 135842 sentence pairs\n",
            "Idioma: fra, Número de palabras palabras: 5228\n",
            "Idioma: eng, Número de palabras: 3434\n",
            "Época 5, Loss average: 2.699308115081219\n",
            "Época 10, Loss average: 1.5062295329777835\n",
            "Época 15, Loss average: 0.994313678226755\n",
            "Época 20, Loss average: 0.6924859017207664\n",
            "Época 25, Loss average: 0.49868581661811245\n",
            "Época 30, Loss average: 0.3690338661741678\n",
            "Época 35, Loss average: 0.2796760972616986\n",
            "Época 40, Loss average: 0.21769682504698892\n",
            "Época 45, Loss average: 0.17657133761987498\n",
            "Época 50, Loss average: 0.14234166374943097\n",
            "Época 55, Loss average: 0.12071543321124376\n",
            "Época 60, Loss average: 0.10420647056833388\n",
            "Época 65, Loss average: 0.09144736236197037\n",
            "Época 70, Loss average: 0.08340819089421564\n",
            "Época 75, Loss average: 0.07560252688880595\n",
            "Época 80, Loss average: 0.06981905049669831\n"
          ]
        }
      ],
      "source": [
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, 128).to(device)\n",
        "decoder = DecoderRNN(128, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0NBTyAYZqww"
      },
      "source": [
        "# EValuación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "YWFA4syTallT"
      },
      "outputs": [],
      "source": [
        "def evaluar(encoder,decoder,oracion,traduccion_real,input_lang,output_lang,length_max):\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  oracion_token=tensorFromSentence(input_lang,oracion) #shape [1,10]: 1 muestra, 10 seq length\n",
        "\n",
        "\n",
        "  #Le paso el vector de tokens al encoder para obtener el hidden state del encoder y pasarle al decoder\n",
        "  output_encoder,hidden_encoder=encoder(oracion_token) #[1,seq_length,dim_hidden] --> [1,10,128]\n",
        "\n",
        "\n",
        "  #Hidden decoder\n",
        "  hidden_decoder=hidden_encoder\n",
        "\n",
        "  #Aquí empieza la predicción del decoder\n",
        "  input_decoder=(torch.zeros(1).unsqueeze(0)).long() #[Batch dimension,seq_length]==[batch dimension,1]==[1,1]\n",
        "  #print(f\"Token inicial: {input_decoder.shape,input_decoder.dtype}\")\n",
        "\n",
        "\n",
        "  #Aquí almaceno los resultados\n",
        "  res=\"\"\n",
        "\n",
        "  for i in range(length_max):\n",
        "    with torch.no_grad():\n",
        "      out_decoder,hidden_decoder=decoder.forward_step(input_decoder,hidden_decoder) #Aquí el input decoder\n",
        "\n",
        "      #print(f\"out decoder shape: {out_decoder.shape}\")\n",
        "\n",
        "      #Conversión de logits de predicción a probs\n",
        "      out_decoder_probs=torch.functional.F.softmax(out_decoder,dim=2)\n",
        "\n",
        "      #Índice de la \"predicción\" (palabra 'siguiente')\n",
        "      index_class_pred=torch.argmax(out_decoder_probs,dim=2)\n",
        "      #print(f\"Clase predicha: {index_class_pred}\")\n",
        "\n",
        "\n",
        "      if index_class_pred.item()==1: #Si es igual al índice de token EOS\n",
        "        break\n",
        "\n",
        "      else:\n",
        "        input_decoder=index_class_pred #Actualizo el siguiente input_decoder\n",
        "        res+=f\" {output_lang.index2word[index_class_pred.item()]}\"\n",
        "\n",
        "\n",
        "  print(\"===\")\n",
        "  print(f\"ORACIÓN: {oracion}\")\n",
        "  print(f\"TRADUCCIÓN REAL: {traduccion_real}\")\n",
        "  print(f\"TRADUCCIÓN: {res}\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oraciones de prueba"
      ],
      "metadata": {
        "id": "szj2xWpHM1QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Oración de prueba en francés\n",
        "oraciones=[\"vous etes inquiete n est ce pas ?\",\n",
        "           \"nous nous rapprochons\",\n",
        "           \"il est de loin le meilleur des etudiants\",\n",
        "           \"elle est affairee\",\n",
        "           \"nous ne sommes pas en securite\",\n",
        "           \"il est rarement de bonne humeur\",\n",
        "           \"je commence a le croire\"]"
      ],
      "metadata": {
        "id": "gRCjynYiKptU"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Traducciones reales de las oraciones\n",
        "oraciones_predichas=[\"you re worried aren t you ?\",\n",
        "           \"we re getting closer\",\n",
        "           \"he is by far the best student\",\n",
        "           \"she s busy with her work\",\n",
        "           \"we re not safe\",\n",
        "           \"he is rarely in a good mood\",\n",
        "           \"i m starting to believe that\"]"
      ],
      "metadata": {
        "id": "X-T2DLE-RnQs"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewO7ufdelR3-",
        "outputId": "16f687a5-c4e7-4239-b9f6-ec8dd4659b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===\n",
            "ORACIÓN: vous etes inquiete n est ce pas ?\n",
            "TRADUCCIÓN REAL: you re worried aren t you ?\n",
            "TRADUCCIÓN:  you re worried aren t you ?\n",
            "===\n",
            "ORACIÓN: nous nous rapprochons\n",
            "TRADUCCIÓN REAL: we re getting closer\n",
            "TRADUCCIÓN:  we re getting out of gas\n",
            "===\n",
            "ORACIÓN: il est de loin le meilleur des etudiants\n",
            "TRADUCCIÓN REAL: he is by far the best student\n",
            "TRADUCCIÓN:  he is by far the best student\n",
            "===\n",
            "ORACIÓN: elle est affairee\n",
            "TRADUCCIÓN REAL: she s busy with her work\n",
            "TRADUCCIÓN:  she s busy with her work\n",
            "===\n",
            "ORACIÓN: nous ne sommes pas en securite\n",
            "TRADUCCIÓN REAL: we re not safe\n",
            "TRADUCCIÓN:  we re not safe here\n",
            "===\n",
            "ORACIÓN: il est rarement de bonne humeur\n",
            "TRADUCCIÓN REAL: he is rarely in a good mood\n",
            "TRADUCCIÓN:  he is rarely in a good mood\n",
            "===\n",
            "ORACIÓN: je commence a le croire\n",
            "TRADUCCIÓN REAL: i m starting to believe that\n",
            "TRADUCCIÓN:  i m beginning to get used to fix about it\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(oraciones)):\n",
        "  oracion=oraciones[i]\n",
        "  traduccion_real=oraciones_predichas[i]\n",
        "  evaluar(encoder,decoder,oracion,traduccion_real,input_lang,output_lang,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn7xH_dZ6vvj"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados:\n",
        "- El modelo predice bien algunas oraciones, pero tiende a fallar en otras. Incluso, cuando se le pasa una sola \"palabra\", devuelve como traducción una 'oración'.\n",
        "# Mejoras:\n",
        "- El modelo, por el momento, consta de una arquitectura Seq2Seq model: encoder, decoder.\n",
        "- No se implementaron attention layers aún, pero le añadiré posteriormente para comparar las mejoras en los resultados."
      ],
      "metadata": {
        "id": "IuqYOiDHSqOD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}